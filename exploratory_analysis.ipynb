{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Choose a labeled text dataset used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Positive    82037\n",
       "Negative    82037\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import  balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('amazon_balanced.csv')\n",
    "df[\"Sentiment\"].value_counts()   # Unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "def print_relevant_words(pipe):\n",
    "    words = pipe['vectorizer'].get_feature_names_out()\n",
    "    print(\"Vocabulary size\", len(words))\n",
    "    coefs = pipe['model'].coef_\n",
    "    classes = pipe['model'].classes_\n",
    "\n",
    "    top_n_words = 10\n",
    "    sorted_coef_indexes = coefs.argsort(axis=1)\n",
    "\n",
    "    negative_words = [(words[i], coefs[0, i]) for i in sorted_coef_indexes[0, :top_n_words]]\n",
    "    positive_words = [(words[i], coefs[0, i]) for i in sorted_coef_indexes[0, -top_n_words:]]\n",
    "\n",
    "    table = []\n",
    "    for neg, pos in zip(negative_words, positive_words):\n",
    "        table.append([f\"{neg[0]} | relevance: {neg[1]:.2f}\", f\"{pos[0]} | relevance: {pos[1]:.2f}\"])\n",
    "\n",
    "    print(tabulate(table, headers=[\"Negative\", \"Positive\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a classification pipeline and 3. Run and evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Review']\n",
    "label_mapping = {'Positive': 1, 'Negative': 0}\n",
    "y = df['Sentiment'].map(label_mapping)   # 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "def get_train_test_accuracy(pipeline_factory, x_vector, y_vector):\n",
    "    pipeline = pipeline_factory()\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_vector, y_vector, test_size=0.2)\n",
    "    # Train the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # Predict on training set\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    train_accuracy = balanced_accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    test_accuracy = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "def get_mean_accurancy(pipeline_factory, x_vector, y_vector, n=10):\n",
    "    results = Parallel(n_jobs=-1)(delayed(get_train_test_accuracy)(pipeline_factory, x_vector, y_vector) for i in range(n))\n",
    "    results = [result for result in results if result is not None]  # Ensure results are not None\n",
    "\n",
    "    train_accuracies = np.array([train_accuracy for train_accuracy, _ in results])\n",
    "    test_accuracies = np.array([test_accuracy for _, test_accuracy in results])\n",
    "    mean_train_accuracies = np.mean(train_accuracies)\n",
    "    mean_test_accuracies = np.mean(test_accuracies)\n",
    "    print(f\"Train accuracy: {mean_train_accuracies:.3f}, Test accuracy: {mean_test_accuracies:.3f} over {n} runs\")\n",
    "\n",
    "    return mean_train_accuracies, mean_test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipe score:  0.8924577175072376\n",
      "Accuracy:  0.892460134037297\n",
      "Vocabulary size 10837\n",
      "+----------------------------------+-------------------------------+\n",
      "| Negative                         | Positive                      |\n",
      "+==================================+===============================+\n",
      "| overrated | relevance: -3.23     | teens | relevance: 1.94       |\n",
      "+----------------------------------+-------------------------------+\n",
      "| dissapointing | relevance: -3.03 | unbeatable | relevance: 1.94  |\n",
      "+----------------------------------+-------------------------------+\n",
      "| unacceptable | relevance: -2.78  | jimmies | relevance: 2.00     |\n",
      "+----------------------------------+-------------------------------+\n",
      "| yuck | relevance: -2.74          | compostable | relevance: 2.01 |\n",
      "+----------------------------------+-------------------------------+\n",
      "| cancelled | relevance: -2.63     | addicting | relevance: 2.01   |\n",
      "+----------------------------------+-------------------------------+\n",
      "| ahold | relevance: -2.49         | emptying | relevance: 2.05    |\n",
      "+----------------------------------+-------------------------------+\n",
      "| ripoff | relevance: -2.46        | yum | relevance: 2.28         |\n",
      "+----------------------------------+-------------------------------+\n",
      "| lacked | relevance: -2.46        | hooked | relevance: 2.28      |\n",
      "+----------------------------------+-------------------------------+\n",
      "| worst | relevance: -2.41         | delish | relevance: 2.29      |\n",
      "+----------------------------------+-------------------------------+\n",
      "| medicinelike | relevance: -2.40  | pleasantly | relevance: 2.65  |\n",
      "+----------------------------------+-------------------------------+\n",
      "Train accuracy: 0.920, Test accuracy: 0.892 over 20 runs\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normal_pipeline_factory():\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"vectorizer\",\n",
    "                CountVectorizer(\n",
    "                    preprocessor=lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x.lower()),\n",
    "                    binary=True,\n",
    "                    stop_words=\"english\",\n",
    "                    min_df=20,\n",
    "                ),\n",
    "            ),\n",
    "            (\"model\", LogisticRegression(max_iter=1000)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "pipe_count_binary = normal_pipeline_factory()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "pipe_count_binary.fit(X_train, y_train)\n",
    "print(\"Pipe score: \", pipe_count_binary.score(X_test, y_test))\n",
    "y_pred = pipe_count_binary.predict(X_test)\n",
    "accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print_relevant_words(pipe_count_binary)\n",
    "n = 20\n",
    "train_accuracy, test_accurancy = get_mean_accurancy(normal_pipeline_factory, X, y, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for arbitray text\n",
    "text_positive = [\"This product is unbelievably good\"]\n",
    "text_negative = [\"This product is unbelievably bad\"]  # For some reason this is classified as positive\n",
    "prediction_positive = pipe_count_binary.predict(text_positive)\n",
    "prediction_negative = pipe_count_binary.predict(text_negative)\n",
    "print(\"Prediction positive: \", prediction_positive)\n",
    "print(\"Prediction negative: \", prediction_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized pipeline (selected as best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "X_lemmatized = X.apply(lambda x: ' '.join([lemmatizer.lemmatize(p) for p in x.lower().split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipe score:  0.8870028950175225\n",
      "Accuracy:  0.8870034456951701\n",
      "Vocabulary size 9612\n",
      "+----------------------------------+--------------------------------+\n",
      "| Negative                         | Positive                       |\n",
      "+==================================+================================+\n",
      "| overrated | relevance: -3.25     | skeptical | relevance: 2.03    |\n",
      "+----------------------------------+--------------------------------+\n",
      "| yikes | relevance: -2.74         | hooked | relevance: 2.07       |\n",
      "+----------------------------------+--------------------------------+\n",
      "| weakest | relevance: -2.74       | yum | relevance: 2.08          |\n",
      "+----------------------------------+--------------------------------+\n",
      "| dissapointing | relevance: -2.72 | nutritionist | relevance: 2.10 |\n",
      "+----------------------------------+--------------------------------+\n",
      "| rediculous | relevance: -2.65    | delish | relevance: 2.17       |\n",
      "+----------------------------------+--------------------------------+\n",
      "| yuck | relevance: -2.65          | addicting | relevance: 2.20    |\n",
      "+----------------------------------+--------------------------------+\n",
      "| worst | relevance: -2.50         | bounce | relevance: 2.23       |\n",
      "+----------------------------------+--------------------------------+\n",
      "| lame | relevance: -2.49          | aging | relevance: 2.29        |\n",
      "+----------------------------------+--------------------------------+\n",
      "| unfinished | relevance: -2.47    | emptying | relevance: 2.42     |\n",
      "+----------------------------------+--------------------------------+\n",
      "| boo | relevance: -2.46           | pleasantly | relevance: 2.87   |\n",
      "+----------------------------------+--------------------------------+\n",
      "Train accuracy: 0.915, Test accuracy: 0.889 over 20 runs\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def lemma_pipeline_factory():\n",
    "    return Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                preprocessor=lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x.lower()),\n",
    "                binary=True,\n",
    "                stop_words=\"english\",\n",
    "                min_df=20,\n",
    "            ),\n",
    "        ),\n",
    "        (\"model\", LogisticRegression(max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_lemmatized, X_test_lemmatized, y_train_lemmatized, y_test_lemmatized = (\n",
    "    train_test_split(X_lemmatized, y, test_size=0.2, random_state=0)\n",
    ")\n",
    "\n",
    "# Print vocabulary size\n",
    "\n",
    "pipe_lemma_count_binary = lemma_pipeline_factory()\n",
    "pipe_lemma_count_binary.fit(X_train_lemmatized, y_train_lemmatized)\n",
    "print(\n",
    "    \"Pipe score: \", pipe_lemma_count_binary.score(X_test_lemmatized, y_test_lemmatized)\n",
    ")\n",
    "y_pred_lemmatized = pipe_lemma_count_binary.predict(X_test_lemmatized)\n",
    "accuracy = balanced_accuracy_score(y_test_lemmatized, y_pred_lemmatized)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print_relevant_words(pipe_lemma_count_binary)\n",
    "n = 20\n",
    "train_accuracy, test_accurancy = get_mean_accurancy(lemma_pipeline_factory, X_lemmatized, y, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmed pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "X_stemmed = X.apply(lambda x: ' '.join([stemmer.stem(p) for p in x.lower().split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipe score:  0.8825841840621667\n",
      "Accuracy:  0.8825882562149723\n",
      "Vocabulary size 7663\n",
      "+-----------------------------+--------------------------------+\n",
      "| Negative                    | Positive                       |\n",
      "+=============================+================================+\n",
      "| boo | relevance: -2.84      | delicaci | relevance: 1.92     |\n",
      "+-----------------------------+--------------------------------+\n",
      "| blech | relevance: -2.72    | scotland | relevance: 1.94     |\n",
      "+-----------------------------+--------------------------------+\n",
      "| weakest | relevance: -2.62  | snickerdoodl | relevance: 1.96 |\n",
      "+-----------------------------+--------------------------------+\n",
      "| yuck | relevance: -2.59     | woof | relevance: 1.97         |\n",
      "+-----------------------------+--------------------------------+\n",
      "| overr | relevance: -2.59    | delish | relevance: 2.06       |\n",
      "+-----------------------------+--------------------------------+\n",
      "| schar | relevance: -2.54    | deduct | relevance: 2.09       |\n",
      "+-----------------------------+--------------------------------+\n",
      "| worst | relevance: -2.51    | downsid | relevance: 2.09      |\n",
      "+-----------------------------+--------------------------------+\n",
      "| putrid | relevance: -2.40   | reassur | relevance: 2.14      |\n",
      "+-----------------------------+--------------------------------+\n",
      "| unfinish | relevance: -2.35 | smoothest | relevance: 2.17    |\n",
      "+-----------------------------+--------------------------------+\n",
      "| yike | relevance: -2.33     | yum | relevance: 2.26          |\n",
      "+-----------------------------+--------------------------------+\n",
      "Train accuracy: 0.905, Test accuracy: 0.883 over 20 runs\n"
     ]
    }
   ],
   "source": [
    "X_train_stemmed, X_test_stemmed, y_train_stemmed, y_test_stemmed = train_test_split(\n",
    "    X_stemmed, y, test_size=0.2\n",
    ")\n",
    "\n",
    "def stem_pipeline_factory():\n",
    "    return Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                preprocessor=lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x.lower()),\n",
    "                binary=True,\n",
    "                stop_words=\"english\",\n",
    "                min_df=20,\n",
    "            ),\n",
    "        ),\n",
    "        (\"model\", LogisticRegression(max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_stemmed_count = stem_pipeline_factory()\n",
    "# Print vocabulary size\n",
    "\n",
    "pipe_stemmed_count.fit(X_train_stemmed, y_train_stemmed)\n",
    "print(\"Pipe score: \", pipe_stemmed_count.score(X_test_stemmed, y_test_stemmed))\n",
    "y_pred_stemmed = pipe_stemmed_count.predict(X_test_stemmed)\n",
    "accuracy = balanced_accuracy_score(y_test_stemmed, y_pred_stemmed)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print_relevant_words(pipe_stemmed_count)\n",
    "n = 20\n",
    "train_accuracy, test_accurancy = get_mean_accurancy(stem_pipeline_factory, X_stemmed, y, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Pipeline: CountVectorizer, Lemmatization and Logistic Regression\n",
    "#### Reasons \n",
    "* Better interpretability\n",
    "* Smaller vocabulary, less dimensionality\n",
    "* Small differences between classifier mean accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Assess the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "f_list = np.logspace(0, -2, 50)\n",
    "\n",
    "# Function to process one fraction f\n",
    "def process_fraction(f):\n",
    "    accuracy_f_train = []\n",
    "    accuracy_f_test = []\n",
    "    for i in range(50):  # Repeat sampling 20 times\n",
    "        df_sample = df.sample(frac=f)\n",
    "        \n",
    "        train_accuracy, test_accuracy = get_train_test_accuracy(lemma_pipeline_factory, df_sample['Plot'], df_sample['Genre'])\n",
    "        \n",
    "        accuracy_f_train.append(train_accuracy)\n",
    "        accuracy_f_test.append(test_accuracy)\n",
    "\n",
    "    # Return the average accuracies for this fraction f\n",
    "    return (sum(accuracy_f_train) / len(accuracy_f_train), \n",
    "            sum(accuracy_f_test) / len(accuracy_f_test))\n",
    "\n",
    "def get_accuracy_sample(df, f_list):\n",
    "    # Resample the dataset using .sample(frac=f), which gets a fraction of the dataset\n",
    "    results = Parallel(n_jobs=-1)(delayed(process_fraction)(f) for f in f_list)\n",
    "    \n",
    "    # Separate the train and test accuracies\n",
    "    train_accuracies, test_accuracies = zip(*results)\n",
    "    \n",
    "    return train_accuracies, test_accuracies\n",
    "\n",
    "train_accuracies, test_accuracies = get_accuracy_sample(df, f_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
