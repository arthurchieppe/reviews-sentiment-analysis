{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Choose a labeled text dataset used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Positive    82037\n",
       "Negative    82037\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import  balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('amazon_balanced.csv')\n",
    "df[\"Sentiment\"].value_counts()   # Unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "def print_relevant_words(pipe):\n",
    "    words = pipe['vectorizer'].get_feature_names_out()\n",
    "    print(\"Vocabulary size\", len(words))\n",
    "    coefs = pipe['model'].coef_\n",
    "    classes = pipe['model'].classes_\n",
    "\n",
    "    top_n_words = 10\n",
    "    sorted_coef_indexes = coefs.argsort(axis=1)\n",
    "\n",
    "    negative_words = [(words[i], coefs[0, i]) for i in sorted_coef_indexes[0, :top_n_words]]\n",
    "    positive_words = [(words[i], coefs[0, i]) for i in sorted_coef_indexes[0, -top_n_words:]]\n",
    "\n",
    "    table = []\n",
    "    for neg, pos in zip(negative_words, positive_words):\n",
    "        table.append([f\"{neg[0]} | relevance: {neg[1]:.2f}\", f\"{pos[0]} | relevance: {pos[1]:.2f}\"])\n",
    "\n",
    "    print(tabulate(table, headers=[\"Negative\", \"Positive\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a classification pipeline and 3. Run and evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Review']\n",
    "label_mapping = {'Positive': 1, 'Negative': 0}\n",
    "y = df['Sentiment'].map(label_mapping)   # 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "def get_train_test_accuracy(pipeline_factory, x_vector, y_vector):\n",
    "    pipeline = pipeline_factory()\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_vector, y_vector, test_size=0.2)\n",
    "    # Train the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # Predict on training set\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    train_accuracy = balanced_accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    test_accuracy = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "def get_mean_accurancy(pipeline_factory, x_vector, y_vector, n=10, n_jobs = -1):\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(get_train_test_accuracy)(pipeline_factory, x_vector, y_vector) for i in range(n))\n",
    "    results = [result for result in results if result is not None]  # Ensure results are not None\n",
    "\n",
    "    train_accuracies = np.array([train_accuracy for train_accuracy, _ in results])\n",
    "    test_accuracies = np.array([test_accuracy for _, test_accuracy in results])\n",
    "    mean_train_accuracies = np.mean(train_accuracies)\n",
    "    mean_test_accuracies = np.mean(test_accuracies)\n",
    "\n",
    "    return mean_train_accuracies, mean_test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "\"i\",\n",
    "\"me\",\n",
    "\"my\",\n",
    "\"myself\",\n",
    "\"we\",\n",
    "\"our\",\n",
    "\"ours\",\n",
    "\"ourselves\",\n",
    "\"you\",\n",
    "\"your\",\n",
    "\"yours\",\n",
    "\"yourself\",\n",
    "\"yourselves\",\n",
    "\"he\",\n",
    "\"him\",\n",
    "\"his\",\n",
    "\"himself\",\n",
    "\"she\",\n",
    "\"her\",\n",
    "\"hers\",\n",
    "\"herself\",\n",
    "\"it\",\n",
    "\"its\",\n",
    "\"itself\",\n",
    "\"they\",\n",
    "\"them\",\n",
    "\"their\",\n",
    "\"theirs\",\n",
    "\"themselves\",\n",
    "\"what\",\n",
    "\"which\",\n",
    "\"who\",\n",
    "\"whom\",\n",
    "\"this\",\n",
    "\"that\",\n",
    "\"these\",\n",
    "\"those\",\n",
    "\"am\",\n",
    "\"is\",\n",
    "\"are\",\n",
    "\"was\",\n",
    "\"were\",\n",
    "\"be\",\n",
    "\"been\",\n",
    "\"being\",\n",
    "\"have\",\n",
    "\"has\",\n",
    "\"had\",\n",
    "\"having\",\n",
    "\"do\",\n",
    "\"does\",\n",
    "\"did\",\n",
    "\"doing\",\n",
    "\"a\",\n",
    "\"an\",\n",
    "\"the\",\n",
    "\"and\",\n",
    "\"but\",\n",
    "\"if\",\n",
    "\"or\",\n",
    "\"because\",\n",
    "\"as\",\n",
    "\"until\",\n",
    "\"while\",\n",
    "\"of\",\n",
    "\"at\",\n",
    "\"by\",\n",
    "\"for\",\n",
    "\"with\",\n",
    "\"about\",\n",
    "\"against\",\n",
    "\"between\",\n",
    "\"into\",\n",
    "\"through\",\n",
    "\"during\",\n",
    "\"before\",\n",
    "\"after\",\n",
    "\"above\",\n",
    "\"below\",\n",
    "\"to\",\n",
    "\"from\",\n",
    "\"up\",\n",
    "\"down\",\n",
    "\"in\",\n",
    "\"out\",\n",
    "\"on\",\n",
    "\"off\",\n",
    "\"over\",\n",
    "\"under\",\n",
    "\"again\",\n",
    "\"further\",\n",
    "\"then\",\n",
    "\"once\",\n",
    "\"here\",\n",
    "\"there\",\n",
    "\"when\",\n",
    "\"where\",\n",
    "\"why\",\n",
    "\"how\",\n",
    "\"all\",\n",
    "\"any\",\n",
    "\"both\",\n",
    "\"each\",\n",
    "\"few\",\n",
    "\"more\",\n",
    "\"most\",\n",
    "\"other\",\n",
    "\"some\",\n",
    "\"such\",\n",
    "\"no\",\n",
    "\"nor\",\n",
    "\"not\",\n",
    "\"only\",\n",
    "\"own\",\n",
    "\"same\",\n",
    "\"so\",\n",
    "\"than\",\n",
    "\"too\",\n",
    "\"very\",\n",
    "\"s\",\n",
    "\"t\",\n",
    "\"can\",\n",
    "\"will\",\n",
    "\"just\",\n",
    "\"don\",\n",
    "\"should\",\n",
    "\"now\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normal_pipeline_factory():\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"vectorizer\",\n",
    "                CountVectorizer(\n",
    "                    preprocessor=lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x.lower()),\n",
    "                    binary=True,\n",
    "                    stop_words=\"english\",\n",
    "                    min_df=20,\n",
    "                ),\n",
    "            ),\n",
    "            (\"model\", LogisticRegression(max_iter=1000)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "pipe_count_binary = normal_pipeline_factory()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "pipe_count_binary.fit(X_train, y_train)\n",
    "print(\"Pipe score: \", pipe_count_binary.score(X_test, y_test))\n",
    "y_pred = pipe_count_binary.predict(X_test)\n",
    "accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print_relevant_words(pipe_count_binary)\n",
    "n = 20\n",
    "train_accuracy, test_accurancy = get_mean_accurancy(normal_pipeline_factory, X, y, n)\n",
    "print(f\"Train accuracy: {train_accuracy:.3f}, Test accuracy: {test_accurancy:.3f} over {n} runs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for arbitray text\n",
    "text_positive = [\"This product is unbelievably good\"]\n",
    "text_negative = [\"This product is unbelievably bad\"]  # For some reason this is classified as positive\n",
    "prediction_positive = pipe_count_binary.predict(text_positive)\n",
    "prediction_negative = pipe_count_binary.predict(text_negative)\n",
    "print(\"Prediction positive: \", prediction_positive)\n",
    "print(\"Prediction negative: \", prediction_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized pipeline (selected as best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "X_lemmatized = X.apply(lambda x: ' '.join([lemmatizer.lemmatize(p) for p in x.lower().split()]))\n",
    "lemmatized_stop_words = [lemmatizer.lemmatize(word) for word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipe score:  0.8898369648026817\n",
      "Accuracy:  0.889837507146106\n",
      "Vocabulary size 9784\n",
      "+----------------------------------+--------------------------------+\n",
      "| Negative                         | Positive                       |\n",
      "+==================================+================================+\n",
      "| overrated | relevance: -3.18     | thati | relevance: 1.97        |\n",
      "+----------------------------------+--------------------------------+\n",
      "| dissapointing | relevance: -2.87 | hooked | relevance: 1.97       |\n",
      "+----------------------------------+--------------------------------+\n",
      "| boo | relevance: -2.72           | nutritionist | relevance: 1.98 |\n",
      "+----------------------------------+--------------------------------+\n",
      "| yuck | relevance: -2.70          | yum | relevance: 2.05          |\n",
      "+----------------------------------+--------------------------------+\n",
      "| yikes | relevance: -2.70         | skeptical | relevance: 2.09    |\n",
      "+----------------------------------+--------------------------------+\n",
      "| weakest | relevance: -2.58       | addicting | relevance: 2.21    |\n",
      "+----------------------------------+--------------------------------+\n",
      "| lame | relevance: -2.55          | aging | relevance: 2.22        |\n",
      "+----------------------------------+--------------------------------+\n",
      "| worst | relevance: -2.53         | delish | relevance: 2.33       |\n",
      "+----------------------------------+--------------------------------+\n",
      "| rediculous | relevance: -2.50    | emptying | relevance: 2.44     |\n",
      "+----------------------------------+--------------------------------+\n",
      "| cancelled | relevance: -2.41     | pleasantly | relevance: 2.85   |\n",
      "+----------------------------------+--------------------------------+\n",
      "Train accuracy: 0.917, Test accuracy: 0.892 over 20 runs\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def lemma_pipeline_factory():\n",
    "    return Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                preprocessor=lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x.lower()),\n",
    "                binary=True,\n",
    "                stop_words=lemmatized_stop_words,\n",
    "                min_df=20,\n",
    "            ),\n",
    "        ),\n",
    "        (\"model\", LogisticRegression(max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_lemmatized, X_test_lemmatized, y_train_lemmatized, y_test_lemmatized = (\n",
    "    train_test_split(X_lemmatized, y, test_size=0.2, random_state=0)\n",
    ")\n",
    "\n",
    "# Print vocabulary size\n",
    "\n",
    "pipe_lemma_count_binary = lemma_pipeline_factory()\n",
    "pipe_lemma_count_binary.fit(X_train_lemmatized, y_train_lemmatized)\n",
    "print(\n",
    "    \"Pipe score: \", pipe_lemma_count_binary.score(X_test_lemmatized, y_test_lemmatized)\n",
    ")\n",
    "y_pred_lemmatized = pipe_lemma_count_binary.predict(X_test_lemmatized)\n",
    "accuracy = balanced_accuracy_score(y_test_lemmatized, y_pred_lemmatized)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print_relevant_words(pipe_lemma_count_binary)\n",
    "n = 20\n",
    "train_accuracy, test_accurancy = get_mean_accurancy(lemma_pipeline_factory, X_lemmatized, y, n)\n",
    "print(f\"Train accuracy: {train_accuracy:.3f}, Test accuracy: {test_accurancy:.3f} over {n} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmed pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "X_stemmed = X.apply(lambda x: ' '.join([stemmer.stem(p) for p in x.lower().split()]))\n",
    "stemmed_stop_words = [stemmer.stem(word) for word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stemmed, X_test_stemmed, y_train_stemmed, y_test_stemmed = train_test_split(\n",
    "    X_stemmed, y, test_size=0.2\n",
    ")\n",
    "\n",
    "def stem_pipeline_factory():\n",
    "    return Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                preprocessor=lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x.lower()),\n",
    "                binary=True,\n",
    "                stop_words=stemmed_stop_words,\n",
    "                min_df=20,\n",
    "            ),\n",
    "        ),\n",
    "        (\"model\", LogisticRegression(max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_stemmed_count = stem_pipeline_factory()\n",
    "# Print vocabulary size\n",
    "\n",
    "pipe_stemmed_count.fit(X_train_stemmed, y_train_stemmed)\n",
    "print(\"Pipe score: \", pipe_stemmed_count.score(X_test_stemmed, y_test_stemmed))\n",
    "y_pred_stemmed = pipe_stemmed_count.predict(X_test_stemmed)\n",
    "accuracy = balanced_accuracy_score(y_test_stemmed, y_pred_stemmed)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print_relevant_words(pipe_stemmed_count)\n",
    "n = 20\n",
    "train_accuracy, test_accurancy = get_mean_accurancy(stem_pipeline_factory, X_stemmed, y, n)\n",
    "print(f\"Train accuracy: {train_accuracy:.3f}, Test accuracy: {test_accurancy:.3f} over {n} runs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Pipeline: CountVectorizer, Lemmatization and Logistic Regression\n",
    "#### Reasons \n",
    "* Better interpretability\n",
    "* Smaller vocabulary, less dimensionality\n",
    "* Small differences between classifier mean accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Assess the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "f_list = np.logspace(0, -2.5, 50)\n",
    "\n",
    "# Function to process one fraction f\n",
    "def process_fraction(x_vector, y_vector, f, n=20):\n",
    "    # Sample the dataset\n",
    "    if float(f) == 1.0:\n",
    "            X_sample, y_sample = x_vector, y_vector\n",
    "    else:\n",
    "        X_sample, _, y_sample, _ = train_test_split(x_vector, y_vector, train_size=float(f))\n",
    "    train_accuracy, test_accuracy = get_mean_accurancy(lemma_pipeline_factory, X_sample, y_sample, n, n_jobs=-1)\n",
    "\n",
    "    # Return the average accuracies for this fraction f\n",
    "    print(\"Processed fraction\", f)\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "def get_accuracy_sample(x_vector, y_vector, f_list):\n",
    "    # Resample the dataset using .sample(frac=f), which gets a fraction of the dataset\n",
    "    results = Parallel(n_jobs=-1)(delayed(process_fraction)(x_vector, y_vector, f) for f in f_list)\n",
    "    \n",
    "    # Separate the train and test accuracies\n",
    "    train_accuracies, test_accuracies = zip(*results)\n",
    "    \n",
    "    return train_accuracies, test_accuracies\n",
    "\n",
    "train_accuracies, test_accuracies = get_accuracy_sample(X_lemmatized, y, f_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "error_train = [1 - x for x in train_accuracies]\n",
    "error_test = [1 - x for x in test_accuracies]\n",
    "\n",
    "plt.plot(f_list * len(df), error_train, label='Train')\n",
    "plt.plot(f_list * len(df), error_test, label='Test')\n",
    "plt.title(\"Training curves\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Use topic models to refine your answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_words_in_topics(nmf, vectorizer):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for idx, topic in enumerate(nmf.components_):\n",
    "        print(f\"Topic {idx}\")\n",
    "        for i in topic.argsort()[-5:]:\n",
    "            print(words[i])\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "vectorizer_nmf_pipeline = Pipeline([\n",
    "            (\"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                strip_accents=\"ascii\",\n",
    "                binary=True,\n",
    "                stop_words=lemmatized_stop_words,\n",
    "                min_df=20,\n",
    "            ),\n",
    "        ),\n",
    "        ('nmf', NMF(n_components=4))])\n",
    "X_nmf = vectorizer_nmf_pipeline.fit_transform(X_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "just\n",
      "good\n",
      "flavor\n",
      "taste\n",
      "like\n",
      "\n",
      "Topic 1\n",
      "ordered\n",
      "amazon\n",
      "box\n",
      "product\n",
      "wa\n",
      "\n",
      "Topic 2\n",
      "cat\n",
      "day\n",
      "eat\n",
      "dont\n",
      "food\n",
      "\n",
      "Topic 3\n",
      "food\n",
      "ha\n",
      "product\n",
      "love\n",
      "great\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_words_in_topics(vectorizer_nmf_pipeline['nmf'], vectorizer_nmf_pipeline['vectorizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wa'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
